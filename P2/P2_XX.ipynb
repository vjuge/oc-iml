{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "498d58ac-6437-4a4a-837f-67296123db71",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "continuer à nettoyer le dataset\n",
    "\n",
    "### 1. analyser les outliers\n",
    "### 2. formater les categories\n",
    "\n",
    "en lowercase, splitter les tags (one hot ?). tip : `.apply(fct)` existe en python, ex: `data['taille'] = data['taille'].apply(convert_height)`\n",
    "\n",
    "### 3. approfondir la distribution des quali et quanti [cours](https://openclassrooms.com/fr/courses/4525266-decrivez-et-nettoyez-votre-jeu-de-donnees/4742171-representez-la-distribution-empirique-dune-variable)\n",
    "\n",
    "### 4. discretiser les variables continues\n",
    "\n",
    "> Le fait d'agréger une variable s'appelle la discrétisation (en anglais : binning, bucketing ou discretization).\n",
    "\n",
    "empirical cumulative distribution function ([ecdf](https://machinelearningmastery.com/empirical-distribution-function-in-python/)) Dans le cas d'une variable qui ne suit pas une loi normale => est-ce le cas dans ce dataset ?\n",
    "\n",
    "$ F_{emp}(x)=\\frac{1}{n}\\sum_{i=1}^{n}I_{\\{{x_{i}} \\leq x\\}}$\n",
    "\n",
    "où I est la fonction indicatrice. 1 si xi <= x, 0 sinon\n",
    "\n",
    "code samples from course:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# VARIABLE QUALITATIVE\n",
    "# Diagramme en secteurs\n",
    "data[\"categ\"].value_counts(normalize=True).plot(kind='pie')\n",
    "# Cette ligne assure que le pie chart est un cercle plutôt qu'une éllipse\n",
    "plt.axis('equal')\n",
    "plt.show() # Affiche le graphique\n",
    "\n",
    "# Diagramme en tuyaux d'orgues\n",
    "data[\"categ\"].value_counts(normalize=True).plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "# VARIABLE QUANTITATIVE\n",
    "# Diagramme en bâtons\n",
    "data[\"quart_mois\"].value_counts(normalize=True).plot(kind='bar',width=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Histogramme\n",
    "data[\"montant\"].hist(density=True)\n",
    "plt.show()\n",
    "# Histogramme plus beau\n",
    "data[data.montant.abs() < 100][\"montant\"].hist(density=True,bins=20)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 5. faire le nombre et la fréquence des valeurs quali (pour toutes celles retenues dans le dataset)\n",
    "\n",
    "```python\n",
    "effectifs = data[\"quart_mois\"].value_counts()\n",
    "modalites = effectifs.index # l'index de effectifs contient les modalités\n",
    "\n",
    "tab = pd.DataFrame(modalites, columns = [\"quart_mois\"]) # création du tableau à partir des modalités\n",
    "tab[\"n\"] = effectifs.values\n",
    "tab[\"f\"] = tab[\"n\"] / len(data) # len(data) renvoie la taille de l'échantillon\n",
    "```\n",
    "\n",
    "> modalité\n",
    ">\n",
    "> effectif\n",
    ">\n",
    "> fréquence\n",
    ">\n",
    "> variable qualitative ordinale\n",
    "\n",
    "### 6. reverifier si la suppression des colonnes à du sens, si ce n'est pas preferable pour certaines de remplir avec une valeur arbitraire (moyenne, autre ...)\n",
    "\n",
    "### 7. remplacer valeur NaN (`fillna`)\n",
    "\n",
    "### 8. resample dataset ?\n",
    "pour diminuer la taille et faciliter les calculs ?  `sklearn.utils.resample`\n",
    "\n",
    "### 9. Analyse ANOVA\n",
    "\n",
    "correlation entre variables\n",
    "\n",
    "### 10. grouper les valeurs\n",
    "\n",
    "par ex sur date `pd.cut` ou  `pd.qcut` pour les regrouper par mois\n",
    "\n",
    "### 11. Re-indexer\n",
    "re-indexer : chaque ligne identifiée avec le nom de l'aliment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1ff66-eda8-4a1b-9de4-8589fb721ecc",
   "metadata": {},
   "source": [
    "Box plot definition\n",
    "![](assets/boxplot.png)\n",
    "\n",
    "![](assets/mad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6de027-bbf9-4ef4-8550-0b3410b91919",
   "metadata": {},
   "source": [
    "## Removal of outliners\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/05/feature-engineering-how-to-detect-and-remove-outliers-with-python-code/\n",
    "\n",
    "https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67e805-4886-4c92-bd23-5e832c5c69d5",
   "metadata": {},
   "source": [
    "### Z-Score\n",
    "\n",
    "![](./assets/Normal_Distribution_deviations.png)\n",
    "\n",
    "\n",
    "applies only for unimodal & normal-distributions values, otherwise is not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7807a5-7dc8-4d87-9b17-6eacff569584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  2.0\n",
      "MSE:  8.0\n",
      "RMSE:  2.8284271247461903\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1, 2])\n",
    "y_pred = np.array([5, 2])\n",
    "print('MAE: ', mean_absolute_error(y, y_pred))\n",
    "print('MSE: ', mean_squared_error(y, y_pred))\n",
    "print('RMSE: ', np.sqrt(mean_squared_error(y, y_pred)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "R^2: 0.999\n",
    "\n",
    "Adj R^2: 0.99\n",
    "\n",
    "RMSE: 0.003\n",
    "\n",
    "MSE: 0.000\n",
    "\n",
    "MAE: 0.003\n",
    "\n",
    "MAPE: 0.001\n",
    "\n",
    "Correlation between actual and predicted: 1.000\n",
    "\n",
    "### Erreur quadratique moyenne - Mean Squared Error\n",
    "$MSE = \\frac{1}{m} \\sum (y_{vrai}-y_{pred})^{2}$\n",
    "\n",
    "### Erreur absolue moyenne - Mean Absolute Error\n",
    "\n",
    "$MAE = \\frac{1}{m} \\sum |y_{vrai}-y_{pred}| $\n",
    "\n",
    "### Root Mean Square Error\n",
    "\n",
    "permet de \"remettre à l'echelle\" notre erreur MSE\n",
    "\n",
    "RMSE = $ \\sqrt{MSE} $ = $ \\sqrt{\\frac{1}{m} \\sum (y_{vrai}-y_{pred})^{2}} $\n",
    "utilisations :\n",
    "\n",
    "MSE quand on veut prendre une importance exponentielle de l'erreur de prediction. la MSE pénalise beaucoup plus les grande erreurs.\n",
    "\n",
    "MAE quand on contraire, qq rares grosses erreurs ne doivent pas être trop impactante dans l'appréciation du modele. quand le dataset comprends qq valeurs aberrantes (outliers)\n",
    "\n",
    "\n",
    "### Median Absolute Error\n",
    "\n",
    "Quand on veut s'affranchir des outliers, et une sensibilité linéaire aux erreurs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}